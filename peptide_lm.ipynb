{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cfd88-e0b0-4a53-9851-994c1282183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "# Define a module for attention blocks\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.masking = masking\n",
    "\n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.25)\n",
    "\n",
    "    def forward(self, x_in, kv_in, key_mask=None):\n",
    "        # Apply causal masking if enabled\n",
    "        if self.masking:\n",
    "            bs, l, h = x_in.shape\n",
    "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
    "        else:\n",
    "            mask = None\n",
    "            \n",
    "        # Perform multi-head attention operation\n",
    "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n",
    "\n",
    "\n",
    "# Define a module for a transformer block with self-attention and optional causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, is_decoder=False, masking=True):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        # Layer normalization for the input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        # Self-attention mechanism\n",
    "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
    "        \n",
    "        # Layer normalization for the output of the first attention layer\n",
    "        if self.is_decoder:\n",
    "            self.norm2 = nn.LayerNorm(hidden_size)\n",
    "            # Self-attention mechanism for the decoder with no masking\n",
    "            self.attn2 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=False)\n",
    "        \n",
    "        # Layer normalization for the output before the MLP\n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "        # Multi-layer perceptron (MLP)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size * 4, hidden_size))\n",
    "                \n",
    "    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n",
    "        # Perform self-attention operation\n",
    "        x = self.attn1(x, x, key_mask=input_key_mask) + x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # If decoder, perform additional cross-attention layer\n",
    "        if self.is_decoder:\n",
    "            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        # Apply MLP and layer normalization\n",
    "        x = self.mlp(x) + x\n",
    "        return self.norm_mlp(x)\n",
    "    \n",
    "    \n",
    "# Define an encoder module for the Transformer architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, is_decoder=False, masking=False) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "    def forward(self, input_seq, padding_mask=None):        \n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, input_key_mask=padding_mask)\n",
    "        \n",
    "        return embs\n",
    "\n",
    "    \n",
    "# Define a decoder module for the Transformer architecture\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, is_decoder=True, masking=True) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "        \n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None, encoder_padding_mask=None):        \n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs,\n",
    "                         input_key_mask=input_padding_mask,\n",
    "                         cross_key_mask=encoder_padding_mask, \n",
    "                         kv_cross=encoder_output)\n",
    "        \n",
    "        return self.fc_out(embs)\n",
    "\n",
    "    \n",
    "# Define an Encoder-Decoder module for the Transformer architecture\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = Encoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[0], num_heads=num_heads)\n",
    "        \n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        # Generate padding masks for input and target sequences\n",
    "        input_key_mask = input_seq == 0\n",
    "        output_key_mask = target_seq == 0\n",
    "\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encoder(input_seq=input_seq, \n",
    "                                   padding_mask=input_key_mask)\n",
    "        \n",
    "        # Decode the target sequence using the encoded sequence\n",
    "        decoded_seq = self.decoder(input_seq=target_seq, \n",
    "                                   encoder_output=encoded_seq, \n",
    "                                   input_padding_mask=output_key_mask, \n",
    "                                   encoder_padding_mask=input_key_mask)\n",
    "\n",
    "        return decoded_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
